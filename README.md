Local LLM Chat App

A simple chat-based local Large Language Model (LLM) application built in Python using Ollama and Streamlit â€” running fully on your machine with no cloud APIs or external servers. Your data stays private, and there are no API fees.

ğŸš€ Overview

This project demonstrates how to:

 - Run an LLM locally using Ollama

 - Build a client UI with Streamlit

 - Maintain chat history in a conversational interface

Itâ€™s a great learning project to understand how local LLM inference works, and can be expanded for privacy-first AI use cases.


ğŸ§  Features

 - Local execution â€” everything runs on your computer

 - Python-based UI using Streamlit

 - Responsive chat interface with streaming output

 - Conversation memory with Streamlitâ€™s session state

 - Works with Llama 3.2 (or other models) via Ollama


ğŸ“¦ Prerequisites

Before getting started, make sure you have:

 - Python 3.8+ installed

 - Ollama installed and initialized on your system

 - At least 8â€“16GB of RAM (recommended for best performance)
   

ğŸ’¡ How It Works

- Streamlit UI â€” Displays a friendly chat interface.

 - Session State â€” Maintains conversation history.

 - Ollama Chat â€” Calls the local model with streaming responses.

 - Typing Effect â€” Uses stream=True for progressive output.

 
 ğŸ“– Resources

 - Ollama â€” Local model management tool

 - Streamlit â€” Python UI framework

 - Llama 3.2 â€” Example LLM used in this project
   

ğŸ™ Credits & Acknowledgments

This project was inspired by and built following the guide:

â€œBuilding Your First Local LLM Appâ€
Author: Aman XAI
Website: https://amanxai.com

The article provides a clear, beginner-friendly walkthrough for running a local LLM using Ollama and Streamlit, and served as the foundational reference for this projectâ€™s structure and implementation. I took it a step further by adding in multi-document search, file upload, and context retrieval!
