Local LLM Chat App

A simple chat-based local Large Language Model (LLM) application built in Python using Ollama and Streamlit â€” running fully on your machine with no cloud APIs or external servers. Your data stays private, and there are no API fees.

ðŸš€ Overview

This project demonstrates how to:

 - Run an LLM locally using Ollama

 - Build a client UI with Streamlit

 - Maintain chat history in a conversational interface

Itâ€™s a great learning project to understand how local LLM inference works, and can be expanded for privacy-first AI use cases.


ðŸ§  Features

 - Local execution â€” everything runs on your computer

 - Python-based UI using Streamlit

 - Responsive chat interface with streaming output

 - Conversation memory with Streamlitâ€™s session state

 - Works with Llama 3.2 (or other models) via Ollama


ðŸ“¦ Prerequisites

Before getting started, make sure you have:

 - Python 3.8+ installed

 - Ollama installed and initialized on your system

 - At least 8â€“16GB of RAM (recommended for best performance)
